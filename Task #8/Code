import http.cookiejar
import re
import urllib.error
import urllib.error
import urllib.parse
import urllib.parse
import urllib.request
import urllib.request
import requests
from bs4 import BeautifulSoup
from lxml import html

top_limit = 10


def openWebsite():
    # enter Github username of user
    username = str(input("absolute-viper: "))

    # Dictionary to store key as repository
    # name and value as no. of stars
    repo_dict = {}

    # This is first page url where user
    # repositories are located
    url = "https://github.com/" + username + "?tab=repositories"

    # loop for all the pages
    while True:

        # open the website and get
        # the html of webpage into doc
        cj = http.cookiejar.CookieJar()
        opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))
        resp = opener.open(url)
        doc = html.fromstring(resp.read())

        # extract all the repository names
        repo_name = doc.xpath('//li[@class="col-12 d-block width-full py-4 border-bottom public source"]'
                              '/div[@class="d-inline-block mb-1"]/h3/a/text()')

        # list to store repository names
        repo_list = []

        # get the repository name
        for name in repo_name:
            name = ' '.join(''.join(name).split())
            repo_list.append(name)
            repo_dict[name] = 0

        # print repo_list
        response = requests.get(url)

        soup = BeautifulSoup(response.text, 'html.parser')
        div = soup.find_all('li', {'class': 'col-12 d-block width-full py-4 border-bottom public source'})

        for d in div:
            temp = d.find_all('div', {'class': 'f6 text-gray mt-2'})
            for t in temp:

                # Get the no. of stars of
                # particular repository
                x = t.find_all('a', attrs={'href': re.compile("^/[a-zA-Z0-9\-_.]+[a-zA-Z0-9.\-_]+\stargazers")})

                # Get the url of the repository
                # and populate the values of dictionary
                # with no. of stars
                if len(x) is not 0:
                    name = x[0].get('href')
                    name = name[len(username) + 2:-11]
                    repo_dict[name] = int(x[0].text)

                # Check if next page exists
        # for more repositories
        div = soup.find('a', {'class': 'next_page'})

        # print div
        if div is not None:
            url = div.get('href')
            url = "https://github.com/" + url
        else:
            # if there is no next repository
            # page, then exit loop
            break


# Driver program
if __name__ == "__main__":
    openWebsite()
